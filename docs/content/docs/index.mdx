---
title: OSS-Ratelimit
description: Production-ready, open-source rate limiting for Node.js & Next.js. Inspired by Upstash Ratelimit with enhanced features and flexibility.
---

import { Card, Cards } from 'fumadocs-ui/components/card';
import { Accordion, Accordions, AccordionItem } from 'fumadocs-ui/components/accordion';
import { Banner } from 'fumadocs-ui/components/banner';
import { Callout } from 'fumadocs-ui/components/callout';
import { File, Folder, Files } from 'fumadocs-ui/components/files';
import { TypeTable } from 'fumadocs-ui/components/type-table';
import { Tab, Tabs } from 'fumadocs-ui/components/tabs';
import { Step, Steps } from 'fumadocs-ui/components/steps';


{/* <Banner>
  Welcome to `oss-ratelimit`! Robust, flexible rate limiting for your applications.
</Banner> */}

`oss-ratelimit` is a feature-rich rate limiting library designed for performance and ease of use, heavily inspired by `@upstash/ratelimit`. It provides multiple algorithms, efficient Redis integration, fallback mechanisms, and a powerful registry system for managing multiple limiter configurations.

**Key Features:**

*   **Multiple Algorithms:** Fixed Window, Sliding Window, and Token Bucket.
*   **Redis Backend:** Leverages Redis for distributed, high-performance state management.
*   **Client Registry:** Efficiently manage multiple rate limiter instances and share Redis connections.
*   **Ephemeral Cache:** Optional in-memory fallback during Redis outages (currently for Sliding Window).
*   **Fail Open/Closed:** Configurable behavior during Redis failures.
*   **Analytics:** Optional tracking of request counts and throughput.
*   **TypeScript First:** Strongly typed for better developer experience.
*   **Next.js Ready:** Includes utilities and patterns for seamless integration (IP detection, middleware).
*   **Customizable:** Flexible configuration options for fine-tuning.

<Cards>
  <Card title="GitHub Repository" href="[Link to your GitHub Repo]" />
  <Card title="npm Package" href="[Link to your npm package]" />
</Cards>

## Getting Started

<Steps>
<Step>

### Installation

Install the library and its peer dependency `redis`:

```bash
npm install oss-ratelimit redis
# or
yarn add oss-ratelimit redis
# or
pnpm add oss-ratelimit redis
```

You might also need types for Redis if not automatically inferred:
```bash
npm install -D @types/redis
# or
yarn add -D @types/redis
# or
pnpm add -D @types/redis
```

</Step>
<Step>

### Basic Setup

The quickest way to get started is by creating a single `Ratelimit` instance. Ensure your Redis server is running or accessible.

```typescript
import { Ratelimit, fixedWindow, getRedisClient } from 'oss-ratelimit';
import { createClient } from 'redis'; // Import redis client

// 1. Prepare Redis Client (using environment variables is recommended)
// Option A: Use the library's helper (handles connection logic)
const redisClientPromise = getRedisClient({
  url: process.env.RATELIMIT_REDIS_URL || 'redis://localhost:6379',
});

// Option B: Provide your own connected client instance
// const myRedisClient = createClient({ url: process.env.RATELIMIT_REDIS_URL });
// await myRedisClient.connect();


// 2. Configure the Limiter Algorithm
const limiterAlgorithm = fixedWindow(
  10,     // Allow 10 requests...
  '15 s'  // ...per 15 seconds
);

// 3. Create the Ratelimit Instance
// We await the client promise here before creating the Ratelimit instance
let ratelimit: Ratelimit;

redisClientPromise.then(redisClient => {
  ratelimit = new Ratelimit({
    redis: redisClient,           // The connected Redis client
    limiter: limiterAlgorithm,    // The chosen algorithm configuration
    prefix: 'myapp_basic',        // Optional: Prefix for Redis keys
  });
  console.log("Rate limiter initialized successfully!");
}).catch(err => {
  console.error("Failed to initialize rate limiter:", err);
  // Handle initialization failure (e.g., exit process, disable features)
});


// Export the instance (or a function to get it) for use elsewhere
// Note: Accessing 'ratelimit' before the promise resolves will result in an error.
// Consider using the registry pattern for easier management (see below).
export const getBasicRatelimiter = async (): Promise<Ratelimit> => {
    await redisClientPromise; // Ensure connection before returning
    if (!ratelimit) throw new Error("Ratelimiter not initialized");
    return ratelimit;
}
```

<Callout>
For managing multiple limiters or simplifying client handling, we **strongly recommend** using the **Client & Registry Management** features described below.
</Callout>

</Step>
<Step>

### Usage

Apply the rate limit using a unique identifier (like an IP address or user ID).

```typescript
import type { NextApiRequest, NextApiResponse } from 'next';
import { getBasicRatelimiter } from '@/lib/ratelimit'; // Adjust path
// You'll need an IP detection function (see Next.js Integration section)
import { getIpFromRequest } from '@/utils/getIpFromRequest';

export default async function handler(req: NextApiRequest, res: NextApiResponse) {
  const ip = getIpFromRequest(req); // Get client IP

  if (!ip) {
    return res.status(400).json({ error: 'Cannot determine IP address.' });
  }

  try {
    const ratelimit = await getBasicRatelimiter(); // Get the initialized instance
    const { success, limit, remaining, reset, retryAfter } = await ratelimit.limit(ip);

    // Set standard rate limit headers
    res.setHeader('X-RateLimit-Limit', limit);
    res.setHeader('X-RateLimit-Remaining', remaining);
    res.setHeader('X-RateLimit-Reset', Math.ceil(reset / 1000)); // Unix timestamp seconds

    if (!success) {
      if (retryAfter) {
         res.setHeader('Retry-After', retryAfter); // Seconds until retry is allowed
      }
      return res.status(429).json({
        error: `Rate limit exceeded for ${ip}. Try again later.`,
        retryAfter: retryAfter ?? 'unknown',
      });
    }

    // --- Rate limit passed, proceed with your logic ---
    res.status(200).json({ message: `Hello from ${ip}! Remaining: ${remaining}` });
    // --- End logic ---

  } catch (error: any) {
     console.error("Ratelimit error:", error);
     // Handle specific errors like Redis connection issues if needed
     if (error.name === 'RedisConnectionError') {
        // Specific handling if Redis is down and failOpen is false
        return res.status(503).json({ error: 'Service temporarily unavailable.' });
     }
     return res.status(500).json({ error: 'Internal Server Error' });
  }
}
```

</Step>
</Steps>

## Core Concepts

### Identifier

The `identifier` is a unique string used to track requests for a specific user, IP address, API key, or any other entity you want to rate limit. This is the primary argument passed to the `.limit()` method.

### Redis

Redis is used as the fast, distributed backend store for tracking request counts or tokens. `oss-ratelimit` uses efficient Lua scripts to perform atomic operations directly on the Redis server, minimizing latency and race conditions.

### Algorithms

`oss-ratelimit` provides several common rate limiting algorithms:

*   **Fixed Window:** Counts requests within discrete time windows (e.g., 10 requests per minute). Simple, but can allow bursts at window edges.
*   **Sliding Window:** Counts requests within a rolling time window. Smoother than Fixed Window, generally preferred for API rate limiting. More resource-intensive.
*   **Token Bucket:** Allows bursts based on accumulated tokens that refill over time. Good for throttling based on average rate while allowing occasional peaks.

Choose the algorithm that best suits your specific use case.

## Algorithms In Depth

<Tabs items={['Fixed Window', 'Sliding Window', 'Token Bucket']}>
  <Tab value="Fixed Window">
    <p>Counts requests within fixed, non-overlapping time intervals.</p>

    **Configuration:**
    ```typescript
    import { fixedWindow } from 'oss-ratelimit';

    const fwLimiter = fixedWindow(
      100,    // Max 100 requests
      '1 h'   // Per hour window (windows reset on the hour)
    );
    ```

    **Use Case:** Simple limits like "max 5 login attempts per 15 minutes".

    **Pros:** Simple to understand and implement, lower Redis resource usage.
    **Cons:** Allows double the rate limit burst at the boundary between two windows.

  </Tab>
  <Tab value="Sliding Window">
     <p>Counts requests within a rolling time window ending at the current request time. Provides smoother limiting.</p>

    **Configuration:**
    ```typescript
    import { slidingWindow } from 'oss-ratelimit';

    const swLimiter = slidingWindow(
      20,    // Max 20 requests
      '10 s' // In any 10-second sliding period
    );
    ```

    **Use Case:** Common for API rate limiting (e.g., "max 1000 API calls per hour"). This is generally the recommended default for stateless services.

    **Pros:** Smoother rate limiting, prevents edge bursts seen in Fixed Window.
    **Cons:** More computationally expensive and uses more Redis memory (stores timestamps).

     <Callout type='info'>
        The Ephemeral Cache fallback mechanism currently only supports the Sliding Window algorithm.
     </Callout>

  </Tab>
  <Tab value="Token Bucket">
    <p>Simulates a bucket holding tokens. Requests consume tokens, and the bucket refills at a constant rate. Allows for bursts up to the bucket's capacity.</p>

    **Configuration:**
    ```typescript
    import { tokenBucket } from 'oss-ratelimit';

    const tbLimiter = tokenBucket(
      5,       // Refill 5 tokens...
      '1 m',   // ...every minute
      20       // Bucket capacity (max burst) is 20 tokens
    );
    ```

     **Use Case:** Throttling background jobs, controlling access to expensive operations, allowing bursts after periods of inactivity.

    **Pros:** Allows bursts, smooth average rate, decoupled refill and consumption.
    **Cons:** Slightly more complex state to manage (tokens + last refill time).

  </Tab>
</Tabs>

## Client & Registry Management

Managing multiple rate limiters (e.g., for different API endpoints or user tiers) and efficiently handling Redis connections is crucial. `oss-ratelimit` provides a powerful registry system for this.

<Files>
  <Folder name="src" defaultOpen>
    <Folder name="lib" defaultOpen>
      <File name="ratelimit.ts" />
    </Folder>
    <Folder name="pages">
      <Folder name="api">
         <File name="users.ts" />
         <File name="posts.ts" />
      </Folder>
    </Folder>
     <Folder name="utils">
      <File name="getIpFromRequest.ts" />
    </Folder>
    <File name="middleware.ts" />
  </Folder>
  <File name="package.json" />
  <File name=".env" />
</Files>

<Steps>
<Step>
### 1. Initialize the Registry (`initRateLimit`)

Create a registry instance, optionally providing default Redis options. Define the names your limiters will use with a type union for type safety.

```typescript
import {
  initRateLimit,
  RateLimitBuilder,
  RegisterConfigParam, // Type for registration config
  slidingWindow,      // Example algorithm
  fixedWindow,
} from 'oss-ratelimit';

// Define names for your different limiters (enhances type safety)
export type LimiterName = 'apiGeneral' | 'apiExpensive' | 'loginAttempts';

// Create the registry instance
// It's recommended to use environment variables for Redis connection details
export const rateLimiterRegistry: RateLimitBuilder<LimiterName> = initRateLimit<LimiterName>({
  // Default Redis options applied to all limiters unless overridden
  defaultRedisOptions: {
    url: process.env.RATELIMIT_REDIS_URL || 'redis://localhost:6379',
    // password: process.env.RATELIMIT_REDIS_PASSWORD,
    // database: 0, // Default database
  },
});

// Define configurations for each named limiter
export const limiterConfigs: Record<LimiterName, RegisterConfigParam> = {
  apiGeneral: {
    limiter: slidingWindow(50, '30 s'), // 50 requests per 30 seconds
    prefix: 'rl_api_gen', // Unique prefix helps organize Redis keys
    analytics: true,      // Enable analytics for this limiter
  },
  apiExpensive: {
    limiter: fixedWindow(5, '1 m'),     // 5 requests per minute
    prefix: 'rl_api_exp',
    timeout: 500,                      // Shorter Redis timeout for this one
  },
  loginAttempts: {
    limiter: fixedWindow(5, '15 m'),    // 5 login attempts per 15 minutes
    prefix: 'rl_login',
    // Example: Using a different Redis DB or instance for sensitive limits
    // envRedisKey: 'LOGIN_REDIS_URL', // Or use 'redis' options:
    // redis: { database: 1 }
  },
};
```

</Step>
<Step>
### 2. Initialize Limiters (Optional but Recommended)

You can eagerly initialize all defined limiters on application startup using `initializeLimiters`. This ensures Redis connections are established and configurations are validated early.

```typescript
import { initializeLimiters } from 'oss-ratelimit';

// Eagerly initialize all limiters defined in limiterConfigs
// Handle the promise appropriately (e.g., in your server startup logic)
export const initializedLimitersPromise = initializeLimiters({
  registry: rateLimiterRegistry,
  configs: limiterConfigs,
  verbose: process.env.NODE_ENV !== 'production', // Log progress in dev
  throwOnError: true, // Recommended: Stop startup if a limiter fails
});

// You can add listeners to the registry *before* initialization
rateLimiterRegistry.on('limiterRegister', ({ name, clientKey }) => {
  console.log(`âœ… Limiter "${name}" registered using Redis client: ${clientKey}`);
});
rateLimiterRegistry.on('redisError', ({ clientKey, error }) => {
   console.error(`âŒ Redis Error for client ${clientKey}:`, error);
});
rateLimiterRegistry.on('limiterError', ({ name, error }) => {
   console.error(`âŒ Failed to initialize limiter "${name}":`, error);
});


// Handle potential initialization errors globally if needed
initializedLimitersPromise.catch(error => {
   console.error("ðŸ’¥ Critical error during rate limiter initialization:", error);
   // Potentially exit the application or disable features relying on rate limiting
   process.exit(1);
});
```

Alternatively, limiters will be initialized automatically (and lazily) the first time `registry.register(name)` or `registry.get(name)` is called for a specific name if not already initialized.

</Step>
<Step>
### 3. Accessing Limiters

Use `registry.get(name)` to retrieve an initialized limiter instance.

```typescript
import type { NextApiRequest, NextApiResponse } from 'next';
import { rateLimiterRegistry, LimiterName } from '@/lib/ratelimit'; // Adjust path
import { getIpFromRequest } from '@/utils/getIpFromRequest';

export default async function handler(req: NextApiRequest, res: NextApiResponse) {
  const ip = getIpFromRequest(req);
  if (!ip) return res.status(400).json({ error: 'Cannot determine IP.' });

  try {
    // Get the specific, pre-initialized limiter instance by name
    const limiter = rateLimiterRegistry.get('apiExpensive'); // Throws if not initialized

    const { success, remaining } = await limiter.limit(ip);

    // Set headers (implementation omitted for brevity)
    // ... set X-RateLimit-* headers ...

    if (!success) {
      // ... return 429 ...
      return res.status(429).json({ error: 'Too many requests for expensive route.' });
    }

    // --- Proceed with expensive logic ---
    res.status(200).json({ result: 'Expensive operation completed!' });

  } catch (error: any) {
    // Handle errors (e.g., if get() throws, or limit() fails)
    console.error("Rate limit error on expensive route:", error);
    return res.status(500).json({ error: 'Internal Server Error' });
  }
}
```

You can also use the helper functions `getInitializedLimiter` or `createLimiterAccessor` for convenience:

```typescript
import { createLimiterAccessor } from 'oss-ratelimit';

// Create a type-safe getter function bound to your registry
// Ensure this is called *after* limiters are likely initialized, or handle errors
export const getLimiter = createLimiterAccessor(rateLimiterRegistry);

// Usage elsewhere:
// import { getLimiter } from '@/lib/ratelimit';
// const generalApiLimiter = getLimiter('apiGeneral');
```


</Step>
<Step>
### 4. Redis Client Reuse

The registry automatically manages and reuses Redis client connections based on the configuration (`RedisOptions` or `envRedisKey`). This prevents creating excessive connections to your Redis server. You can inspect which client a limiter is using via the `limiterRegister` event or programmatically if needed (though usually not required).

</Step>
</Steps>

## Advanced Usage

### Blocking (`block`)

Wait until a request is allowed, up to a configurable maximum time.

```typescript
import { RateLimitExceededError } from 'oss-ratelimit';

const identifier = 'user_abc';
const limiter = rateLimiterRegistry.get('apiGeneral');

try {
  // Wait up to 3 seconds for the rate limit to allow the request
  const response = await limiter.block(identifier, {
    maxWaitMs: 3000, // Default 5000ms
    // maxAttempts: 10, // Default 50
    // retryDelayMs: 50 // Default 100ms (base delay, backs off automatically)
  });
  console.log(`Request allowed after waiting. Remaining: ${response.remaining}`);
  // Proceed with action...

} catch (error) {
  if (error instanceof RateLimitExceededError) {
    console.warn(`Rate limit exceeded for ${error.identifier} even after waiting. Retry after ${error.retryAfter}s.`);
    // Handle timeout (e.g., return an error to the user)
  } else {
    console.error("Error during block:", error);
    // Handle other errors
  }
}
```

### Resetting Limits (`reset`)

Manually clear the rate limit count for a specific identifier. Useful for testing or specific application logic.

```typescript
const identifierToReset = 'user_xyz';
const limiter = rateLimiterRegistry.get('loginAttempts');

try {
  const success = await limiter.reset(identifierToReset);
  if (success) {
    console.log(`Rate limit reset successfully for ${identifierToReset}`);
  } else {
     console.warn(`Rate limit reset failed for ${identifierToReset} (maybe due to Redis issue and failOpen=true?)`);
  }
} catch (error) {
   console.error(`Failed to reset rate limit for ${identifierToReset}:`, error);
}
```

### Checking Status (`getStats`, `check`)

Query the current state of the rate limit for an identifier *without* consuming a request token/count.

```typescript
const identifier = 'ip_1.2.3.4';
const limiter = rateLimiterRegistry.get('apiGeneral');

// Get detailed stats
const stats = await limiter.getStats(identifier);
console.log(`Stats for ${identifier}: Used=${stats.used}, Remaining=${stats.remaining}, Limit=${stats.limit}, ResetsAt=${new Date(stats.reset).toISOString()}`);

// Simple check if allowed
const isAllowed = await limiter.check(identifier);
if (isAllowed) {
  console.log(`${identifier} has requests remaining.`);
} else {
  console.log(`${identifier} is currently rate limited.`);
}
```

## Configuration Options

Configure the `Ratelimit` instance or registry registration:

<TypeTable
  type={{
    redis: { description: 'A connected `RedisClientType` instance or `RedisOptions` / `envRedisKey` (via registry) to create one.', type: 'RedisClientType | RedisOptions | { envRedisKey: string }' },
    limiter: { description: 'The rate limiting algorithm configuration object created by `fixedWindow`, `slidingWindow`, or `tokenBucket`.', type: 'FixedWindowOptions | SlidingWindowOptions | TokenBucketOptions' },
    prefix: { description: 'Optional prefix for all Redis keys used by this instance.', type: 'string', default: 'open-ratelimit' },
    analytics: { description: 'Enable collection of analytics data (pending requests, throughput). Adds slight overhead.', type: 'boolean', default: 'false' },
    timeout: { description: 'Max time in milliseconds to wait for Redis operations.', type: 'number', default: '1000' },
    ephemeralCache: { description: 'Use an in-memory cache as fallback if Redis fails (Sliding Window only).', type: 'boolean', default: 'true' },
    ephemeralCacheTTL: { description: 'TTL in milliseconds for ephemeral cache entries.', type: 'number', default: '60000' },
    failOpen: { description: 'If true, allow requests when Redis is unavailable. If false, block requests.', type: 'boolean', default: 'false' },
    silent: { description: 'Suppress console warnings (e.g., for ephemeral cache usage).', type: 'boolean', default: 'false' },
  }}
/>

### Redis Options

Passed directly to `redis.createClient` or used by `getRedisClient`:

<TypeTable
  type={{
    url: { description: 'Redis connection string (e.g., `redis[s]://[[username][:password]@][host][:port][/db]`). Takes precedence over individual fields if provided.', type: 'string' },
    host: { description: 'Redis host.', type: 'string' },
    port: { description: 'Redis port.', type: 'number' },
    username: { description: 'Redis username (for ACL).', type: 'string' },
    password: { description: 'Redis password.', type: 'string' },
    database: { description: 'Redis database number.', type: 'number' },
    tls: { description: 'Enable TLS connection.', type: 'boolean' },
    connectTimeout: { description: 'Connection timeout in ms.', type: 'number', default: '5000' },
    reconnectStrategy: { description: 'Custom reconnection strategy function or delay in ms (or false to disable).', type: 'number | false | ((retries: number, cause: Error) => number | false | Error)' },
  }}
/>


## Error Handling

`oss-ratelimit` throws specific errors and emits events.

### Custom Error Types

*   **`RatelimitError`**: Base class for all library errors.
*   **`RedisConnectionError`**: Thrown when connecting to or communicating with Redis fails (and `failOpen` is false).
*   **`RateLimitExceededError`**: Thrown by `block()` if the rate limit is still exceeded after the max wait time. Contains `retryAfter` and `identifier`.

### Handling Errors

Use `try...catch` blocks and check error types:

```typescript
import { RatelimitError, RedisConnectionError, RateLimitExceededError } from 'oss-ratelimit';

// Inside an async function where you call limiter methods...
try {
   const result = await limiter.limit(identifier);
   // ... handle success ...
} catch (error) {
   if (error instanceof RateLimitExceededError) {
      // Specific handling for block() timeout
      console.error(`Blocking failed for ${error.identifier}, retry after ${error.retryAfter}s`);
      // Return 429 or appropriate error
   } else if (error instanceof RedisConnectionError) {
      // Specific handling if Redis is down (and failOpen: false)
      console.error(`Redis connection error: ${error.message}`);
      // Return 503 Service Unavailable or similar
   } else if (error instanceof RatelimitError) {
      // Catch other library-specific errors
       console.error(`Rate limiting operational error: ${error.message}`);
       // Return 500
   } else {
      // Handle unexpected errors
      console.error(`Unexpected error: ${error}`);
      // Return 500
   }
}
```

### Events

The `Ratelimit` instance and the `RateLimitBuilder` (registry) are EventEmitters.

**Ratelimit Instance Events:**
*   `error`: Emitted when an operational error occurs (e.g., Redis issue). Passes the error object.
*   `allowed`: Emitted when a `.limit()` call succeeds. Passes `{ identifier, remaining, limit }`.
*   `limited`: Emitted when a `.limit()` call fails due to rate limiting. Passes `{ identifier, remaining, limit }`.
*   `failOpen`: Emitted when Redis fails but `failOpen: true` allows the request. Passes `{ identifier, error }`.
*   `waiting`: Emitted by `block()` on each retry attempt. Passes `{ identifier, attempt, waitTime, elapsed }`.
*   `reset`: Emitted when `.reset()` successfully clears a limit. Passes `{ identifier }`.

**Registry (`RateLimitBuilder`) Events:**
*   `redisConnect`: Emitted when a managed Redis client connects successfully. Passes `{ clientKey }`.
*   `redisError`: Emitted when a managed Redis client encounters an error. Passes `{ clientKey, error }`.
*   `limiterRegister`: Emitted when a limiter instance is successfully created and registered. Passes `{ name, clientKey }`.
*   `limiterError`: Emitted when registering or initializing a limiter fails. Passes `{ name, error }`.
*   `close`: Emitted when the registry's `close()` method is called.

```typescript
import { rateLimiterRegistry } from '@/lib/ratelimit';

rateLimiterRegistry.on('redisError', ({ clientKey, error }) => {
  // Log Redis errors centrally, maybe send to monitoring
  console.error(`[Registry] Redis Client Error (${clientKey}):`, error);
});

rateLimiterRegistry.on('limiterError', ({ name, error }) => {
   console.error(`[Registry] Limiter Init Error (${name}):`, error);
});
```


## Ephemeral Cache

When `ephemeralCache: true` (the default), the library maintains a simple in-memory cache. If Redis becomes unavailable during a `.limit()` call for a **Sliding Window** limiter, it will fall back to this cache to provide *approximate* rate limiting.

<Callout type="warn" title="Limitations">
*   **Sliding Window Only:** Currently only works with the `slidingWindow` algorithm. Other algorithms will either throw an error or fail open if Redis fails, depending on `failOpen`.
*   **Not Distributed:** The cache is local to each process/instance. It won't work correctly for distributed systems behind a load balancer unless you implement sticky sessions (which often negates the benefit).
*   **Less Precise:** It provides basic counting within a window but lacks the precision and atomic guarantees of Redis.
*   **Memory Usage:** Stores counters in memory, potentially consuming more RAM under high load or with many unique identifiers.
</Callout>

It's primarily useful as a basic resilience mechanism for single-instance deployments or during brief Redis hiccups when using Sliding Window. For robust distributed systems, focus on Redis high availability.


## Analytics

Set `analytics: true` in the configuration to enable tracking of additional metrics.

When enabled, the `RatelimitResponse` from `.limit()` will include:

*   `pending`: (Approximation) The current number of requests counted within the window or tokens used (depends on algorithm).
*   `throughput`: (Approximation) Requests seen in the last second for the specific identifier (calculated via Redis ZCOUNT on a secondary key).

<Callout>
Enabling analytics adds a small overhead due to extra Redis commands (`ZADD`, `ZCOUNT`, `PEXPIRE` on the analytics key). Only enable it if you need these specific metrics per request.
</Callout>

## Next.js Integration

`oss-ratelimit` works well with Next.js. The key is obtaining a reliable identifier (usually the client's IP address) and applying the limit in the appropriate place (API Routes, Middleware, Route Handlers).

### 1. Getting the Client IP Address

Reliably getting the *real* client IP behind proxies requires checking specific headers. Use a dedicated utility function for this.

{/* <Accordion title="Recommended `getIpFromRequest` Utility">
<AccordionItem> */}
```typescript
import { NextApiRequest } from 'next';
import { NextRequest } from 'next/server';
import { IncomingMessage } from 'http';
import ipaddr from 'ipaddr.js'; // Optional: yarn add ipaddr.js @types/ipaddr.js

// Type representing possible request objects
type RequestLike =
  | NextApiRequest
  | NextRequest
  | IncomingMessage
  | { headers: Headers | NodeJS.Dict<string | string[]>; ip?: string; socket?: { remoteAddress?: string }; connection?: { remoteAddress?: string } };

type HeadersLike = Headers | NodeJS.Dict<string | string[]>;

// Helper to safely get header values
function getHeader(headers: HeadersLike | undefined, name: string): string | null {
    if (!headers) return null;
    const lowerCaseName = name.toLowerCase();
    if (headers instanceof Headers) return headers.get(lowerCaseName);
    const headerValue = headers[lowerCaseName] ?? headers[name];
    if (Array.isArray(headerValue)) return headerValue[0];
    if (typeof headerValue === 'string') return headerValue;
    return null;
}

// Optional IP validation
function isValidIp(ip: string | null | undefined): ip is string {
    if (!ip) return false;
    // Basic check if ipaddr.js is not installed
    if (typeof ipaddr === 'undefined') return true;
    try {
        ipaddr.parse(ip);
        return true;
    } catch (e) { return false; }
}

/**
 * Extracts the client's IP address from various request types in Next.js.
 * Checks standard and platform-specific headers, handles proxies, falls back.
 */
export function getIpFromRequest(
    req: RequestLike,
    options: { trustHeaders?: string[]; validateIp?: boolean; } = {}
): string | null {
    const {
        trustHeaders = [ // Prioritize headers based on your infra
            'cf-connecting-ip',       // Cloudflare
            'x-vercel-forwarded-for', // Vercel
            'x-real-ip',              // Nginx, Common Proxies
            'x-forwarded-for',        // Standard, but handle carefully
        ],
        validateIp = typeof ipaddr !== 'undefined',
    } = options;

    const headers = 'headers' in req ? req.headers : undefined;

    // 1. Check NextRequest's 'ip' property (App Router / Middleware - often reliable)
    if ('ip' in req && req.ip && (!validateIp || isValidIp(req.ip))) {
         return req.ip;
    }

    // 2. Check trusted headers
    if (headers) {
        for (const headerName of trustHeaders) {
            let ip = getHeader(headers, headerName);
            if (ip) {
                if (headerName.toLowerCase() === 'x-forwarded-for') {
                    const ips = ip.split(',').map(s => s.trim()).filter(Boolean);
                    ip = ips[0]; // Leftmost is typically the original client
                }
                if (ip && (!validateIp || isValidIp(ip))) return ip;
            }
        }
    }

    // 3. Fallback to direct connection (less reliable behind proxies)
    let directIp: string | undefined | null = null;
    if ('socket' in req && req.socket?.remoteAddress) {
        directIp = req.socket.remoteAddress;
    } else if ('connection' in req && req.connection?.remoteAddress) {
        directIp = req.connection.remoteAddress;
    }
    // Clean '::ffff:' prefix for IPv4 mapped addresses
    if (directIp?.startsWith('::ffff:')) directIp = directIp.substring(7);
    if (directIp && (!validateIp || isValidIp(directIp))) return directIp;

    // 4. Last check on req.ip if validation initially failed
     if ('ip' in req && req.ip && (!validateIp || isValidIp(req.ip))) {
        return req.ip;
    }

    return null; // Could not determine IP
}
```
{/* </AccordionItem> */}
Install `ipaddr.js` for validation: `npm install ipaddr.js`
{/* </Accordion> */}

### 2. Applying Limits

<Tabs items={['API Route (Pages)', 'Middleware', 'Route Handler (App)']}>
  <Tab value="API Route (Pages)">
    Use the pattern shown in the "Basic Usage" example above. Get the IP using `getIpFromRequest(req)`, get your limiter instance from the registry, call `.limit(ip)`, and handle the response (setting headers, returning 429 on failure).
    ```typescript
    import type { NextApiRequest, NextApiResponse } from 'next';
    import { rateLimiterRegistry } from '@/lib/ratelimit';
    import { getIpFromRequest } from '@/utils/getIpFromRequest';

    export default async function handler(req: NextApiRequest, res: NextApiResponse) {
      const ip = getIpFromRequest(req);
      if (!ip) return res.status(400).json({ error: 'Cannot determine IP.' });

      try {
        const limiter = rateLimiterRegistry.get('apiGeneral'); // Or await register
        const { success, limit, remaining, reset, retryAfter } = await limiter.limit(ip);

        res.setHeader('X-RateLimit-Limit', limit);
        res.setHeader('X-RateLimit-Remaining', remaining);
        res.setHeader('X-RateLimit-Reset', Math.ceil(reset / 1000));

        if (!success) {
          if(retryAfter) res.setHeader('Retry-After', retryAfter);
          return res.status(429).json({ error: 'Too Many Requests' });
        }

        // SUCCESS: Proceed with API logic
        res.status(200).json({ data: 'Super secret stuff' });

      } catch (error) {
        console.error("Ratelimit error in API route:", error);
        return res.status(500).json({ error: 'Internal Server Error' });
      }
    }
    ```
  </Tab>
  <Tab value="Middleware">
    Middleware is ideal for applying limits broadly before requests hit your main logic.

    <Callout type="danger" title="Edge Runtime Compatibility">
      Next.js Middleware often runs in the Edge Runtime. `oss-ratelimit` currently uses `node-redis`, which is **NOT** compatible with the Edge Runtime. To use rate limiting in Edge Middleware, you would need:
      1.  A Redis client compatible with the Edge (e.g., `@upstash/redis`).
      2.  Either adapt `oss-ratelimit` to accept/use such a client OR use a different library designed for the Edge (like `@upstash/ratelimit`).

      The following example assumes your middleware runs in the **Node.js runtime** or you've made the necessary adaptations for the Edge.
    </Callout>

    ```typescript
    import { NextResponse } from 'next/server';
    import type { NextRequest } from 'next/server';
    import { getIpFromRequest } from '@/utils/getIpFromRequest';
    import { rateLimiterRegistry, LimiterName } from '@/lib/ratelimit';
    // Ensure your registry initialization is compatible with where middleware runs

    export const config = {
      // Apply middleware to specific paths
      matcher: ['/api/public/:path*', '/dashboard/:path*'],
    };

    export async function middleware(request: NextRequest) {
      const ip = getIpFromRequest(request);
      if (!ip) {
        // Decide how to handle: block, allow, redirect?
        return new NextResponse(JSON.stringify({ error: 'IP unavailable' }), { status: 400 });
      }

      try {
        // Choose limiter based on path, etc.
        const limiterName: LimiterName = request.nextUrl.pathname.startsWith('/api')
          ? 'apiGeneral'
          : 'apiExpensive'; // Example logic

        // Get or initialize limiter (ensure registry is accessible)
        const limiter = await rateLimiterRegistry.register(limiterName);

        const { success, limit, remaining, reset, retryAfter } = await limiter.limit(ip);

        // Prepare headers for response or continuation
        const headers = new Headers(request.headers);
        headers.set('X-RateLimit-Limit', String(limit));
        headers.set('X-RateLimit-Remaining', String(remaining));
        headers.set('X-RateLimit-Reset', String(Math.ceil(reset / 1000)));
        headers.set('X-Client-IP', ip); // Forward identified IP

        if (!success) {
          if(retryAfter) headers.set('Retry-After', String(retryAfter));
          return new NextResponse(
            JSON.stringify({ error: 'Rate limit exceeded' }),
            { status: 429, headers }
          );
        }

        // Rate limit passed, continue request with updated headers
        return NextResponse.next({
          request: { headers },
        });

      } catch (error) {
        console.error("Ratelimit error in middleware:", error);
        // Handle failOpen/failClosed scenarios based on config and error type
        return new NextResponse(JSON.stringify({ error: 'Server Error' }), { status: 500 });
      }
    }
    ```
  </Tab>
  <Tab value="Route Handler (App)">
    Similar to Pages API Routes, but using `NextRequest` and `NextResponse`.

    ```typescript
    import { NextRequest, NextResponse } from 'next/server';
    import { rateLimiterRegistry } from '@/lib/ratelimit';
    import { getIpFromRequest } from '@/utils/getIpFromRequest';

    export async function GET(request: NextRequest) {
      const ip = getIpFromRequest(request);
      if (!ip) {
        return NextResponse.json({ error: 'Cannot determine IP.' }, { status: 400 });
      }

      try {
        const limiter = rateLimiterRegistry.get('apiGeneral'); // Or await register
        const { success, limit, remaining, reset, retryAfter } = await limiter.limit(ip);

        const headers = new Headers();
        headers.set('X-RateLimit-Limit', String(limit));
        headers.set('X-RateLimit-Remaining', String(remaining));
        headers.set('X-RateLimit-Reset', String(Math.ceil(reset / 1000)));

        if (!success) {
          if(retryAfter) headers.set('Retry-After', String(retryAfter));
          return NextResponse.json(
              { error: 'Too Many Requests' },
              { status: 429, headers }
          );
        }

        // SUCCESS: Proceed with route handler logic
        return NextResponse.json({ data: ['item1', 'item2'] }, { status: 200, headers });

      } catch (error) {
        console.error("Ratelimit error in route handler:", error);
        return NextResponse.json({ error: 'Internal Server Error' }, { status: 500 });
      }
    }
    ```
  </Tab>
</Tabs>

## Best Practices

*   **Secure Your Redis:** Use strong passwords, network isolation (firewalls), and consider TLS if connecting over untrusted networks. Do not expose Redis directly to the internet.
*   **Use the Registry:** For any non-trivial application, use `initRateLimit` and the registry pattern to manage instances and Redis connections efficiently.
*   **Choose the Right Algorithm:** Select the algorithm that best matches the behavior you want to enforce (bursts vs. smooth limits). Sliding Window is often a good default for APIs.
*   **Meaningful Identifiers:** Use IP addresses for anonymous users. For authenticated users, prefer User IDs or API Keys for more granular and fair limiting.
*   **Configure `failOpen` Carefully:** Understand the implications. `failOpen: true` prioritizes availability over strict rate limiting during Redis issues, which might be acceptable for some non-critical limits but dangerous for others (like login attempts). `failOpen: false` (default) is safer but impacts availability if Redis fails.
*   **Monitor Redis:** Keep an eye on Redis performance (latency, memory usage, CPU). Rate limiting can put significant load on Redis.
*   **Handle Errors Gracefully:** Implement proper `try...catch` blocks and handle `RedisConnectionError` and other potential issues. Inform users appropriately (e.g., with 429 or 503 status codes).
*   **Set Response Headers:** Include `X-RateLimit-Limit`, `X-RateLimit-Remaining`, `X-RateLimit-Reset`, and `Retry-After` headers to help clients understand their limits.
*   **Test Thoroughly:** Test your rate limiting logic under various load conditions and failure scenarios (e.g., simulating Redis downtime).
*   **Consider Distributed Systems:** If running multiple instances of your app, remember that the Ephemeral Cache is local. Only Redis provides truly distributed state.

## Cleanup

If you manually manage Redis clients or use the registry, ensure you close connections gracefully on application shutdown.

```typescript
import { rateLimiterRegistry } from '@/lib/ratelimit'; // Your registry instance
import { closeRedisClient } from 'oss-ratelimit'; // If using standalone client

async function shutdown() {
  console.log("Shutting down...");

  // Close clients managed by the registry
  await rateLimiterRegistry.close();
  console.log("Rate limiter registry closed.");

  // If you created standalone clients with getRedisClient, close them too
  // await closeRedisClient(); // This closes the *last* client created by getRedisClient

  process.exit(0);
}

process.on('SIGTERM', shutdown);
process.on('SIGINT', shutdown);
```

**Next Steps:**

1.  **Replace Placeholders:** Fill in `[Link to your GitHub Repo]` and `[Link to your npm package]`.
2.  **Review Content:** Read through carefully to ensure accuracy, clarity, and completeness based on your library's exact implementation.
3.  **Add More Detail:** You might want to expand on specific algorithm nuances, provide more complex usage examples, or detail the analytics data further.
4.  **API Reference Page (Optional):** Consider creating a separate page using `<TypeTable>` extensively to formally document every exported function, class, type, and interface for power users.
5.  **Test Fumadocs Components:** Ensure all imports are correct and components render as expected within your Fumadocs setup. Adjust imports if necessary based on your project structure.